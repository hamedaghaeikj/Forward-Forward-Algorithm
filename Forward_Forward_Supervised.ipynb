{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nriMyAZWRL_2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import relu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYIkq53Qo8h_"
      },
      "source": [
        "# Data loading and Transformers\n",
        "\n",
        "## Transformers:\n",
        "* ToTensor(): This transformation converts images to the torch.Tensor() format.\n",
        "* Normalize(): This transformation normalizes images with a specified mean and standard deviation.\n",
        "* Lambda(): This transformation applies an arbitrary function to the images.\n",
        "\n",
        "## Data Loading:\n",
        "\n",
        "In this section, the training and test data from the MNIST dataset were loaded. The batch size was also reduced to 10,000 and 1,000 to address the memory issue and DataLoader() was used to create data batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgP55gqxRL_5",
        "outputId": "6b21a6c2-1a59-4b74-edbe-3c65be1fb689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 80406971.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 32615964.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 19500280.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 7430003.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "transform = torchvision.transforms.Compose([transforms.ToTensor(),\n",
        "                                            transforms.Normalize((0.1,),(0.1,)),\n",
        "                                               transforms.Lambda(lambda x: torch.flatten(x))])\n",
        "train_data = MNIST(root = \"./\", train= True, transform = transform ,download=True)\n",
        "test_data = MNIST(root = \"./\", train= False, transform = transform ,download=False)\n",
        "\n",
        "train_size = 10000\n",
        "test_size = 1000\n",
        "train_loader = DataLoader(dataset=train_data, batch_size=train_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_data, batch_size=test_size, shuffle=True)\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM4sEqxjp9I2"
      },
      "source": [
        "# Learning Class\n",
        "\n",
        "The Learning class is a custom neural network layer that implements the forward-forward algorithm. The class has two main methods: forward() and learn().\n",
        "\n",
        "## Forward Method\n",
        "\n",
        "The forward() method computes the output of the layer for a given input. The input is first normalized and then passed through a ReLU activation function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def forward(self, x):\n",
        "  x_norm = torch.norm(x, 2, 1, keepdim=True)\n",
        "  x_ = x/(x_norm + self.eps)\n",
        "  return relu((x_ @ self.w.T) + self.b.unsqueeze(dim = 0))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "The x_norm is x L2 norm and epsilon is a value to avoid division by zero.\n",
        "\n",
        "## Learn Method\n",
        "\n",
        "The learn() method trains the layer's weights and biases using the forward-forward algorithm. The method takes two input tensors: x_pos and x_neg, representing positive and negative examples, respectively. The method performs the following steps for each epoch:\n",
        "\n",
        "*   Compute the goodness for positive and negative examples and Concatenate the positive and negative goodness values and compute the loss using the loss function.\n",
        "$$ \\log(1 + e^{[(threshold - g_{pos})\\, ,\\, (g_{neg} - threshold)]})$$\n",
        "*   Compute the mean square error (MSE)\n",
        "\n",
        "*   Zero the gradients of the layer's parameters and Compute the backward (derivatives) pass using the loss function.\n",
        "\n",
        "*   Update the layer's parameters using the optimizer.\n",
        "\n",
        "*   Print the current epoch and loss value and Return the updated outputs for positive and negative examples.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "# Answer the question\n",
        "The necessary conditions for goodness for positive and negative data are as follows:\n",
        "\n",
        "*   For positive data, goodness must be positive.\n",
        "\n",
        "*   For negative data, goodness must be negative.\n",
        "\n",
        "\n",
        "The loss function defined above can satisfy these conditions by using the exponantial function. If goodness for positive data is positive, the exp function will map it to a large value whose log is also positive. If goodness for negative data is negative, the exp function will map it to a small value whose log is also negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YcJHwHIsRL_5"
      },
      "outputs": [],
      "source": [
        "class Learning(nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_epochs = 10 ,threshold = 2.0, lr =0.01):\n",
        "        super().__init__()\n",
        "        self.w = torch.nn.Parameter(torch.randn(out_features, in_features))\n",
        "        self.b = torch.nn.Parameter(torch.randn(out_features))\n",
        "        self.lr = lr\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr = self.lr)\n",
        "        self.threshold = threshold\n",
        "        self.num_epochs = num_epochs\n",
        "        self.eps = 1e-4\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_norm = torch.norm(x, 2, 1, keepdim=True)\n",
        "        x_ = x/(x_norm + self.eps)\n",
        "        return relu((x_ @ self.w.T) + self.b.unsqueeze(dim = 0))\n",
        "\n",
        "    def learn(self, x_pos, x_neg):\n",
        "        for epoch in range(self.num_epochs):\n",
        "            g_pos = torch.pow(self.forward(x_pos), 2).mean(dim = 1)\n",
        "            g_neg = torch.pow(self.forward(x_neg),2).mean(dim = 1)\n",
        "\n",
        "            loss = torch.log(1 + torch.exp(torch.cat([self.threshold - g_pos ,\n",
        "                                                       g_neg - self.threshold]))).mean()\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            print(f\"{epoch+1}/{self.num_epochs} --- Loss: {loss.item():.4f}\")\n",
        "\n",
        "        return  (\n",
        "            self.forward(x_pos).detach(),\n",
        "              self.forward(x_neg).detach()\n",
        "              )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJfDHgFCqkM4"
      },
      "source": [
        "# FF class\n",
        "\n",
        "Learning class defines the forward-forward model for the problem. It first creates the layers according to the **Learning** class described above, based on the number of neurons (hidden_dims) it takes.\n",
        "\n",
        "The methods are implemented:\n",
        "\n",
        "* Generate_data( ): This method generates the desired data (positive and negative data) by placing the data in the first ten pixels of the image.\n",
        "* Predict( ): In this method, the prediction is performed. First, a dataset is created for each label based on the data passing through the layers and calculating the amount of Goodness. Then the amount of goodness for each label is calculated. And finally, the prediction is made on the data based on which label has the higher goodness value.\n",
        "* learn( ): Learning takes positive and negative data as argument in this method and use the learn method that implemented FFnet class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V91hJdxsRL_5"
      },
      "outputs": [],
      "source": [
        "class FF(nn.Module):\n",
        "    def __init__(self, hidden_dims, device = 'cpu', num_epochs = 10, lr = 0.01, threshold = 2.0):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.device = device\n",
        "        self.lr = lr\n",
        "        for i in range(len(hidden_dims)-1):\n",
        "            self.layers.append(Learning(hidden_dims[i], hidden_dims[i+1], num_epochs = num_epochs, lr = self.lr, threshold = threshold).to(self.device))\n",
        "        self.num_class = 10\n",
        "        print(f\"---- Device: {device} ----\")\n",
        "\n",
        "    def Generate_data(self, x, y):\n",
        "        x_s = x.clone()\n",
        "        x_s[range(x.shape[0]),:self.num_class] = 0\n",
        "        x_s[range(x.shape[0]),y] = x.max()\n",
        "        return x_s.to(self.device)\n",
        "\n",
        "    def predict(self, x):\n",
        "        label_goodness = []\n",
        "        for label in range(self.num_class):\n",
        "            a = self.Generate_data(x, label)\n",
        "            goodness = []\n",
        "            for layer in self.layers:\n",
        "              a = layer(a)\n",
        "              goodness.append(torch.pow(a , 2).mean(dim = 1))\n",
        "            label_goodness.append(sum(goodness).unsqueeze(dim = 1))\n",
        "        label_goodness = torch.cat(label_goodness, dim = 1)\n",
        "        return torch.argmax(label_goodness, dim = 1)\n",
        "\n",
        "    def learn(self, x_pos, x_neg):\n",
        "        for inx in range(len(self.layers)):\n",
        "            print()\n",
        "            print(f\" Layer {inx +1 } : \")\n",
        "            x_pos, x_neg = self.layers[inx].learn(x_pos, x_neg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxFAUBUJtvw6"
      },
      "source": [
        "# Main\n",
        "\n",
        "\n",
        "*   Set Hyperparameters\n",
        "*   Learn model\n",
        "*   evaluate model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRvRElHkRL_6",
        "outputId": "f018025b-35f4-4fc4-9106-8a567bdd8a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- Device: cuda ----\n",
            "\n",
            " Layer 1 : \n",
            "1/350 --- Loss: 0.8168\n",
            "2/350 --- Loss: 1.9658\n",
            "3/350 --- Loss: 0.6979\n",
            "4/350 --- Loss: 0.8691\n",
            "5/350 --- Loss: 0.9771\n",
            "6/350 --- Loss: 1.0041\n",
            "7/350 --- Loss: 0.9937\n",
            "8/350 --- Loss: 0.9549\n",
            "9/350 --- Loss: 0.8870\n",
            "10/350 --- Loss: 0.7951\n",
            "11/350 --- Loss: 0.7127\n",
            "12/350 --- Loss: 0.7125\n",
            "13/350 --- Loss: 0.7989\n",
            "14/350 --- Loss: 0.8333\n",
            "15/350 --- Loss: 0.7802\n",
            "16/350 --- Loss: 0.7151\n",
            "17/350 --- Loss: 0.6949\n",
            "18/350 --- Loss: 0.7128\n",
            "19/350 --- Loss: 0.7382\n",
            "20/350 --- Loss: 0.7527\n",
            "21/350 --- Loss: 0.7517\n",
            "22/350 --- Loss: 0.7374\n",
            "23/350 --- Loss: 0.7161\n",
            "24/350 --- Loss: 0.6974\n",
            "25/350 --- Loss: 0.6911\n",
            "26/350 --- Loss: 0.7004\n",
            "27/350 --- Loss: 0.7147\n",
            "28/350 --- Loss: 0.7192\n",
            "29/350 --- Loss: 0.7100\n",
            "30/350 --- Loss: 0.6962\n",
            "31/350 --- Loss: 0.6885\n",
            "32/350 --- Loss: 0.6897\n",
            "33/350 --- Loss: 0.6954\n",
            "34/350 --- Loss: 0.7000\n",
            "35/350 --- Loss: 0.7003\n",
            "36/350 --- Loss: 0.6962\n",
            "37/350 --- Loss: 0.6900\n",
            "38/350 --- Loss: 0.6852\n",
            "39/350 --- Loss: 0.6843\n",
            "40/350 --- Loss: 0.6869\n",
            "41/350 --- Loss: 0.6897\n",
            "42/350 --- Loss: 0.6893\n",
            "43/350 --- Loss: 0.6858\n",
            "44/350 --- Loss: 0.6820\n",
            "45/350 --- Loss: 0.6802\n",
            "46/350 --- Loss: 0.6807\n",
            "47/350 --- Loss: 0.6817\n",
            "48/350 --- Loss: 0.6818\n",
            "49/350 --- Loss: 0.6803\n",
            "50/350 --- Loss: 0.6778\n",
            "51/350 --- Loss: 0.6758\n",
            "52/350 --- Loss: 0.6750\n",
            "53/350 --- Loss: 0.6752\n",
            "54/350 --- Loss: 0.6752\n",
            "55/350 --- Loss: 0.6741\n",
            "56/350 --- Loss: 0.6722\n",
            "57/350 --- Loss: 0.6704\n",
            "58/350 --- Loss: 0.6695\n",
            "59/350 --- Loss: 0.6690\n",
            "60/350 --- Loss: 0.6684\n",
            "61/350 --- Loss: 0.6671\n",
            "62/350 --- Loss: 0.6655\n",
            "63/350 --- Loss: 0.6640\n",
            "64/350 --- Loss: 0.6629\n",
            "65/350 --- Loss: 0.6621\n",
            "66/350 --- Loss: 0.6611\n",
            "67/350 --- Loss: 0.6597\n",
            "68/350 --- Loss: 0.6580\n",
            "69/350 --- Loss: 0.6566\n",
            "70/350 --- Loss: 0.6554\n",
            "71/350 --- Loss: 0.6543\n",
            "72/350 --- Loss: 0.6528\n",
            "73/350 --- Loss: 0.6512\n",
            "74/350 --- Loss: 0.6497\n",
            "75/350 --- Loss: 0.6483\n",
            "76/350 --- Loss: 0.6469\n",
            "77/350 --- Loss: 0.6455\n",
            "78/350 --- Loss: 0.6438\n",
            "79/350 --- Loss: 0.6421\n",
            "80/350 --- Loss: 0.6405\n",
            "81/350 --- Loss: 0.6390\n",
            "82/350 --- Loss: 0.6374\n",
            "83/350 --- Loss: 0.6356\n",
            "84/350 --- Loss: 0.6339\n",
            "85/350 --- Loss: 0.6322\n",
            "86/350 --- Loss: 0.6306\n",
            "87/350 --- Loss: 0.6289\n",
            "88/350 --- Loss: 0.6271\n",
            "89/350 --- Loss: 0.6253\n",
            "90/350 --- Loss: 0.6235\n",
            "91/350 --- Loss: 0.6217\n",
            "92/350 --- Loss: 0.6199\n",
            "93/350 --- Loss: 0.6181\n",
            "94/350 --- Loss: 0.6163\n",
            "95/350 --- Loss: 0.6144\n",
            "96/350 --- Loss: 0.6126\n",
            "97/350 --- Loss: 0.6108\n",
            "98/350 --- Loss: 0.6089\n",
            "99/350 --- Loss: 0.6071\n",
            "100/350 --- Loss: 0.6052\n",
            "101/350 --- Loss: 0.6033\n",
            "102/350 --- Loss: 0.6015\n",
            "103/350 --- Loss: 0.5996\n",
            "104/350 --- Loss: 0.5977\n",
            "105/350 --- Loss: 0.5959\n",
            "106/350 --- Loss: 0.5940\n",
            "107/350 --- Loss: 0.5922\n",
            "108/350 --- Loss: 0.5903\n",
            "109/350 --- Loss: 0.5885\n",
            "110/350 --- Loss: 0.5866\n",
            "111/350 --- Loss: 0.5848\n",
            "112/350 --- Loss: 0.5830\n",
            "113/350 --- Loss: 0.5812\n",
            "114/350 --- Loss: 0.5794\n",
            "115/350 --- Loss: 0.5776\n",
            "116/350 --- Loss: 0.5758\n",
            "117/350 --- Loss: 0.5740\n",
            "118/350 --- Loss: 0.5723\n",
            "119/350 --- Loss: 0.5705\n",
            "120/350 --- Loss: 0.5688\n",
            "121/350 --- Loss: 0.5670\n",
            "122/350 --- Loss: 0.5653\n",
            "123/350 --- Loss: 0.5636\n",
            "124/350 --- Loss: 0.5619\n",
            "125/350 --- Loss: 0.5602\n",
            "126/350 --- Loss: 0.5585\n",
            "127/350 --- Loss: 0.5569\n",
            "128/350 --- Loss: 0.5552\n",
            "129/350 --- Loss: 0.5536\n",
            "130/350 --- Loss: 0.5520\n",
            "131/350 --- Loss: 0.5504\n",
            "132/350 --- Loss: 0.5488\n",
            "133/350 --- Loss: 0.5472\n",
            "134/350 --- Loss: 0.5456\n",
            "135/350 --- Loss: 0.5441\n",
            "136/350 --- Loss: 0.5425\n",
            "137/350 --- Loss: 0.5410\n",
            "138/350 --- Loss: 0.5394\n",
            "139/350 --- Loss: 0.5379\n",
            "140/350 --- Loss: 0.5364\n",
            "141/350 --- Loss: 0.5349\n",
            "142/350 --- Loss: 0.5334\n",
            "143/350 --- Loss: 0.5320\n",
            "144/350 --- Loss: 0.5305\n",
            "145/350 --- Loss: 0.5290\n",
            "146/350 --- Loss: 0.5276\n",
            "147/350 --- Loss: 0.5262\n",
            "148/350 --- Loss: 0.5247\n",
            "149/350 --- Loss: 0.5233\n",
            "150/350 --- Loss: 0.5219\n",
            "151/350 --- Loss: 0.5205\n",
            "152/350 --- Loss: 0.5192\n",
            "153/350 --- Loss: 0.5178\n",
            "154/350 --- Loss: 0.5164\n",
            "155/350 --- Loss: 0.5151\n",
            "156/350 --- Loss: 0.5138\n",
            "157/350 --- Loss: 0.5124\n",
            "158/350 --- Loss: 0.5111\n",
            "159/350 --- Loss: 0.5098\n",
            "160/350 --- Loss: 0.5085\n",
            "161/350 --- Loss: 0.5072\n",
            "162/350 --- Loss: 0.5059\n",
            "163/350 --- Loss: 0.5047\n",
            "164/350 --- Loss: 0.5034\n",
            "165/350 --- Loss: 0.5022\n",
            "166/350 --- Loss: 0.5009\n",
            "167/350 --- Loss: 0.4997\n",
            "168/350 --- Loss: 0.4985\n",
            "169/350 --- Loss: 0.4973\n",
            "170/350 --- Loss: 0.4961\n",
            "171/350 --- Loss: 0.4949\n",
            "172/350 --- Loss: 0.4937\n",
            "173/350 --- Loss: 0.4925\n",
            "174/350 --- Loss: 0.4914\n",
            "175/350 --- Loss: 0.4902\n",
            "176/350 --- Loss: 0.4891\n",
            "177/350 --- Loss: 0.4879\n",
            "178/350 --- Loss: 0.4868\n",
            "179/350 --- Loss: 0.4857\n",
            "180/350 --- Loss: 0.4846\n",
            "181/350 --- Loss: 0.4835\n",
            "182/350 --- Loss: 0.4824\n",
            "183/350 --- Loss: 0.4813\n",
            "184/350 --- Loss: 0.4802\n",
            "185/350 --- Loss: 0.4791\n",
            "186/350 --- Loss: 0.4781\n",
            "187/350 --- Loss: 0.4770\n",
            "188/350 --- Loss: 0.4760\n",
            "189/350 --- Loss: 0.4749\n",
            "190/350 --- Loss: 0.4739\n",
            "191/350 --- Loss: 0.4729\n",
            "192/350 --- Loss: 0.4718\n",
            "193/350 --- Loss: 0.4708\n",
            "194/350 --- Loss: 0.4698\n",
            "195/350 --- Loss: 0.4688\n",
            "196/350 --- Loss: 0.4679\n",
            "197/350 --- Loss: 0.4669\n",
            "198/350 --- Loss: 0.4659\n",
            "199/350 --- Loss: 0.4649\n",
            "200/350 --- Loss: 0.4640\n",
            "201/350 --- Loss: 0.4630\n",
            "202/350 --- Loss: 0.4621\n",
            "203/350 --- Loss: 0.4611\n",
            "204/350 --- Loss: 0.4602\n",
            "205/350 --- Loss: 0.4593\n",
            "206/350 --- Loss: 0.4584\n",
            "207/350 --- Loss: 0.4575\n",
            "208/350 --- Loss: 0.4566\n",
            "209/350 --- Loss: 0.4557\n",
            "210/350 --- Loss: 0.4548\n",
            "211/350 --- Loss: 0.4539\n",
            "212/350 --- Loss: 0.4530\n",
            "213/350 --- Loss: 0.4521\n",
            "214/350 --- Loss: 0.4513\n",
            "215/350 --- Loss: 0.4504\n",
            "216/350 --- Loss: 0.4495\n",
            "217/350 --- Loss: 0.4487\n",
            "218/350 --- Loss: 0.4478\n",
            "219/350 --- Loss: 0.4470\n",
            "220/350 --- Loss: 0.4462\n",
            "221/350 --- Loss: 0.4453\n",
            "222/350 --- Loss: 0.4445\n",
            "223/350 --- Loss: 0.4437\n",
            "224/350 --- Loss: 0.4429\n",
            "225/350 --- Loss: 0.4421\n",
            "226/350 --- Loss: 0.4413\n",
            "227/350 --- Loss: 0.4405\n",
            "228/350 --- Loss: 0.4397\n",
            "229/350 --- Loss: 0.4389\n",
            "230/350 --- Loss: 0.4382\n",
            "231/350 --- Loss: 0.4374\n",
            "232/350 --- Loss: 0.4366\n",
            "233/350 --- Loss: 0.4359\n",
            "234/350 --- Loss: 0.4351\n",
            "235/350 --- Loss: 0.4344\n",
            "236/350 --- Loss: 0.4336\n",
            "237/350 --- Loss: 0.4329\n",
            "238/350 --- Loss: 0.4321\n",
            "239/350 --- Loss: 0.4314\n",
            "240/350 --- Loss: 0.4307\n",
            "241/350 --- Loss: 0.4299\n",
            "242/350 --- Loss: 0.4292\n",
            "243/350 --- Loss: 0.4285\n",
            "244/350 --- Loss: 0.4278\n",
            "245/350 --- Loss: 0.4271\n",
            "246/350 --- Loss: 0.4264\n",
            "247/350 --- Loss: 0.4257\n",
            "248/350 --- Loss: 0.4250\n",
            "249/350 --- Loss: 0.4243\n",
            "250/350 --- Loss: 0.4236\n",
            "251/350 --- Loss: 0.4229\n",
            "252/350 --- Loss: 0.4223\n",
            "253/350 --- Loss: 0.4216\n",
            "254/350 --- Loss: 0.4209\n",
            "255/350 --- Loss: 0.4203\n",
            "256/350 --- Loss: 0.4196\n",
            "257/350 --- Loss: 0.4190\n",
            "258/350 --- Loss: 0.4183\n",
            "259/350 --- Loss: 0.4177\n",
            "260/350 --- Loss: 0.4170\n",
            "261/350 --- Loss: 0.4164\n",
            "262/350 --- Loss: 0.4157\n",
            "263/350 --- Loss: 0.4151\n",
            "264/350 --- Loss: 0.4145\n",
            "265/350 --- Loss: 0.4138\n",
            "266/350 --- Loss: 0.4132\n",
            "267/350 --- Loss: 0.4126\n",
            "268/350 --- Loss: 0.4120\n",
            "269/350 --- Loss: 0.4114\n",
            "270/350 --- Loss: 0.4108\n",
            "271/350 --- Loss: 0.4102\n",
            "272/350 --- Loss: 0.4096\n",
            "273/350 --- Loss: 0.4090\n",
            "274/350 --- Loss: 0.4084\n",
            "275/350 --- Loss: 0.4078\n",
            "276/350 --- Loss: 0.4072\n",
            "277/350 --- Loss: 0.4066\n",
            "278/350 --- Loss: 0.4060\n",
            "279/350 --- Loss: 0.4054\n",
            "280/350 --- Loss: 0.4049\n",
            "281/350 --- Loss: 0.4043\n",
            "282/350 --- Loss: 0.4037\n",
            "283/350 --- Loss: 0.4031\n",
            "284/350 --- Loss: 0.4026\n",
            "285/350 --- Loss: 0.4020\n",
            "286/350 --- Loss: 0.4015\n",
            "287/350 --- Loss: 0.4009\n",
            "288/350 --- Loss: 0.4004\n",
            "289/350 --- Loss: 0.3998\n",
            "290/350 --- Loss: 0.3993\n",
            "291/350 --- Loss: 0.3987\n",
            "292/350 --- Loss: 0.3982\n",
            "293/350 --- Loss: 0.3976\n",
            "294/350 --- Loss: 0.3971\n",
            "295/350 --- Loss: 0.3966\n",
            "296/350 --- Loss: 0.3960\n",
            "297/350 --- Loss: 0.3955\n",
            "298/350 --- Loss: 0.3950\n",
            "299/350 --- Loss: 0.3944\n",
            "300/350 --- Loss: 0.3939\n",
            "301/350 --- Loss: 0.3934\n",
            "302/350 --- Loss: 0.3929\n",
            "303/350 --- Loss: 0.3924\n",
            "304/350 --- Loss: 0.3919\n",
            "305/350 --- Loss: 0.3913\n",
            "306/350 --- Loss: 0.3908\n",
            "307/350 --- Loss: 0.3903\n",
            "308/350 --- Loss: 0.3898\n",
            "309/350 --- Loss: 0.3893\n",
            "310/350 --- Loss: 0.3888\n",
            "311/350 --- Loss: 0.3883\n",
            "312/350 --- Loss: 0.3878\n",
            "313/350 --- Loss: 0.3873\n",
            "314/350 --- Loss: 0.3869\n",
            "315/350 --- Loss: 0.3864\n",
            "316/350 --- Loss: 0.3859\n",
            "317/350 --- Loss: 0.3854\n",
            "318/350 --- Loss: 0.3849\n",
            "319/350 --- Loss: 0.3844\n",
            "320/350 --- Loss: 0.3840\n",
            "321/350 --- Loss: 0.3835\n",
            "322/350 --- Loss: 0.3830\n",
            "323/350 --- Loss: 0.3825\n",
            "324/350 --- Loss: 0.3821\n",
            "325/350 --- Loss: 0.3816\n",
            "326/350 --- Loss: 0.3811\n",
            "327/350 --- Loss: 0.3807\n",
            "328/350 --- Loss: 0.3802\n",
            "329/350 --- Loss: 0.3797\n",
            "330/350 --- Loss: 0.3793\n",
            "331/350 --- Loss: 0.3788\n",
            "332/350 --- Loss: 0.3784\n",
            "333/350 --- Loss: 0.3779\n",
            "334/350 --- Loss: 0.3775\n",
            "335/350 --- Loss: 0.3770\n",
            "336/350 --- Loss: 0.3766\n",
            "337/350 --- Loss: 0.3761\n",
            "338/350 --- Loss: 0.3757\n",
            "339/350 --- Loss: 0.3752\n",
            "340/350 --- Loss: 0.3748\n",
            "341/350 --- Loss: 0.3743\n",
            "342/350 --- Loss: 0.3739\n",
            "343/350 --- Loss: 0.3734\n",
            "344/350 --- Loss: 0.3730\n",
            "345/350 --- Loss: 0.3726\n",
            "346/350 --- Loss: 0.3721\n",
            "347/350 --- Loss: 0.3717\n",
            "348/350 --- Loss: 0.3713\n",
            "349/350 --- Loss: 0.3708\n",
            "350/350 --- Loss: 0.3704\n",
            "\n",
            " Layer 2 : \n",
            "1/350 --- Loss: 0.8316\n",
            "2/350 --- Loss: 0.6917\n",
            "3/350 --- Loss: 0.6768\n",
            "4/350 --- Loss: 0.6394\n",
            "5/350 --- Loss: 0.6370\n",
            "6/350 --- Loss: 0.6126\n",
            "7/350 --- Loss: 0.5852\n",
            "8/350 --- Loss: 0.5809\n",
            "9/350 --- Loss: 0.5715\n",
            "10/350 --- Loss: 0.5461\n",
            "11/350 --- Loss: 0.5300\n",
            "12/350 --- Loss: 0.5215\n",
            "13/350 --- Loss: 0.5086\n",
            "14/350 --- Loss: 0.4940\n",
            "15/350 --- Loss: 0.4860\n",
            "16/350 --- Loss: 0.4812\n",
            "17/350 --- Loss: 0.4723\n",
            "18/350 --- Loss: 0.4626\n",
            "19/350 --- Loss: 0.4565\n",
            "20/350 --- Loss: 0.4519\n",
            "21/350 --- Loss: 0.4463\n",
            "22/350 --- Loss: 0.4411\n",
            "23/350 --- Loss: 0.4372\n",
            "24/350 --- Loss: 0.4337\n",
            "25/350 --- Loss: 0.4298\n",
            "26/350 --- Loss: 0.4262\n",
            "27/350 --- Loss: 0.4229\n",
            "28/350 --- Loss: 0.4197\n",
            "29/350 --- Loss: 0.4168\n",
            "30/350 --- Loss: 0.4142\n",
            "31/350 --- Loss: 0.4117\n",
            "32/350 --- Loss: 0.4089\n",
            "33/350 --- Loss: 0.4065\n",
            "34/350 --- Loss: 0.4044\n",
            "35/350 --- Loss: 0.4023\n",
            "36/350 --- Loss: 0.4000\n",
            "37/350 --- Loss: 0.3979\n",
            "38/350 --- Loss: 0.3961\n",
            "39/350 --- Loss: 0.3943\n",
            "40/350 --- Loss: 0.3924\n",
            "41/350 --- Loss: 0.3907\n",
            "42/350 --- Loss: 0.3891\n",
            "43/350 --- Loss: 0.3875\n",
            "44/350 --- Loss: 0.3859\n",
            "45/350 --- Loss: 0.3844\n",
            "46/350 --- Loss: 0.3830\n",
            "47/350 --- Loss: 0.3816\n",
            "48/350 --- Loss: 0.3802\n",
            "49/350 --- Loss: 0.3789\n",
            "50/350 --- Loss: 0.3777\n",
            "51/350 --- Loss: 0.3764\n",
            "52/350 --- Loss: 0.3752\n",
            "53/350 --- Loss: 0.3740\n",
            "54/350 --- Loss: 0.3729\n",
            "55/350 --- Loss: 0.3717\n",
            "56/350 --- Loss: 0.3707\n",
            "57/350 --- Loss: 0.3696\n",
            "58/350 --- Loss: 0.3686\n",
            "59/350 --- Loss: 0.3675\n",
            "60/350 --- Loss: 0.3666\n",
            "61/350 --- Loss: 0.3656\n",
            "62/350 --- Loss: 0.3647\n",
            "63/350 --- Loss: 0.3637\n",
            "64/350 --- Loss: 0.3628\n",
            "65/350 --- Loss: 0.3619\n",
            "66/350 --- Loss: 0.3611\n",
            "67/350 --- Loss: 0.3602\n",
            "68/350 --- Loss: 0.3594\n",
            "69/350 --- Loss: 0.3586\n",
            "70/350 --- Loss: 0.3578\n",
            "71/350 --- Loss: 0.3570\n",
            "72/350 --- Loss: 0.3562\n",
            "73/350 --- Loss: 0.3555\n",
            "74/350 --- Loss: 0.3547\n",
            "75/350 --- Loss: 0.3540\n",
            "76/350 --- Loss: 0.3533\n",
            "77/350 --- Loss: 0.3526\n",
            "78/350 --- Loss: 0.3519\n",
            "79/350 --- Loss: 0.3512\n",
            "80/350 --- Loss: 0.3505\n",
            "81/350 --- Loss: 0.3499\n",
            "82/350 --- Loss: 0.3492\n",
            "83/350 --- Loss: 0.3486\n",
            "84/350 --- Loss: 0.3479\n",
            "85/350 --- Loss: 0.3473\n",
            "86/350 --- Loss: 0.3467\n",
            "87/350 --- Loss: 0.3461\n",
            "88/350 --- Loss: 0.3455\n",
            "89/350 --- Loss: 0.3450\n",
            "90/350 --- Loss: 0.3444\n",
            "91/350 --- Loss: 0.3438\n",
            "92/350 --- Loss: 0.3433\n",
            "93/350 --- Loss: 0.3427\n",
            "94/350 --- Loss: 0.3422\n",
            "95/350 --- Loss: 0.3416\n",
            "96/350 --- Loss: 0.3411\n",
            "97/350 --- Loss: 0.3406\n",
            "98/350 --- Loss: 0.3401\n",
            "99/350 --- Loss: 0.3396\n",
            "100/350 --- Loss: 0.3391\n",
            "101/350 --- Loss: 0.3386\n",
            "102/350 --- Loss: 0.3381\n",
            "103/350 --- Loss: 0.3376\n",
            "104/350 --- Loss: 0.3372\n",
            "105/350 --- Loss: 0.3367\n",
            "106/350 --- Loss: 0.3362\n",
            "107/350 --- Loss: 0.3358\n",
            "108/350 --- Loss: 0.3353\n",
            "109/350 --- Loss: 0.3349\n",
            "110/350 --- Loss: 0.3345\n",
            "111/350 --- Loss: 0.3340\n",
            "112/350 --- Loss: 0.3336\n",
            "113/350 --- Loss: 0.3332\n",
            "114/350 --- Loss: 0.3328\n",
            "115/350 --- Loss: 0.3324\n",
            "116/350 --- Loss: 0.3319\n",
            "117/350 --- Loss: 0.3315\n",
            "118/350 --- Loss: 0.3311\n",
            "119/350 --- Loss: 0.3308\n",
            "120/350 --- Loss: 0.3304\n",
            "121/350 --- Loss: 0.3300\n",
            "122/350 --- Loss: 0.3296\n",
            "123/350 --- Loss: 0.3292\n",
            "124/350 --- Loss: 0.3288\n",
            "125/350 --- Loss: 0.3285\n",
            "126/350 --- Loss: 0.3281\n",
            "127/350 --- Loss: 0.3278\n",
            "128/350 --- Loss: 0.3274\n",
            "129/350 --- Loss: 0.3270\n",
            "130/350 --- Loss: 0.3267\n",
            "131/350 --- Loss: 0.3263\n",
            "132/350 --- Loss: 0.3260\n",
            "133/350 --- Loss: 0.3257\n",
            "134/350 --- Loss: 0.3253\n",
            "135/350 --- Loss: 0.3250\n",
            "136/350 --- Loss: 0.3247\n",
            "137/350 --- Loss: 0.3243\n",
            "138/350 --- Loss: 0.3240\n",
            "139/350 --- Loss: 0.3237\n",
            "140/350 --- Loss: 0.3234\n",
            "141/350 --- Loss: 0.3231\n",
            "142/350 --- Loss: 0.3228\n",
            "143/350 --- Loss: 0.3224\n",
            "144/350 --- Loss: 0.3221\n",
            "145/350 --- Loss: 0.3218\n",
            "146/350 --- Loss: 0.3215\n",
            "147/350 --- Loss: 0.3212\n",
            "148/350 --- Loss: 0.3209\n",
            "149/350 --- Loss: 0.3206\n",
            "150/350 --- Loss: 0.3203\n",
            "151/350 --- Loss: 0.3201\n",
            "152/350 --- Loss: 0.3198\n",
            "153/350 --- Loss: 0.3195\n",
            "154/350 --- Loss: 0.3192\n",
            "155/350 --- Loss: 0.3189\n",
            "156/350 --- Loss: 0.3187\n",
            "157/350 --- Loss: 0.3184\n",
            "158/350 --- Loss: 0.3181\n",
            "159/350 --- Loss: 0.3178\n",
            "160/350 --- Loss: 0.3176\n",
            "161/350 --- Loss: 0.3173\n",
            "162/350 --- Loss: 0.3170\n",
            "163/350 --- Loss: 0.3168\n",
            "164/350 --- Loss: 0.3165\n",
            "165/350 --- Loss: 0.3162\n",
            "166/350 --- Loss: 0.3160\n",
            "167/350 --- Loss: 0.3157\n",
            "168/350 --- Loss: 0.3155\n",
            "169/350 --- Loss: 0.3152\n",
            "170/350 --- Loss: 0.3150\n",
            "171/350 --- Loss: 0.3147\n",
            "172/350 --- Loss: 0.3145\n",
            "173/350 --- Loss: 0.3142\n",
            "174/350 --- Loss: 0.3140\n",
            "175/350 --- Loss: 0.3138\n",
            "176/350 --- Loss: 0.3135\n",
            "177/350 --- Loss: 0.3133\n",
            "178/350 --- Loss: 0.3130\n",
            "179/350 --- Loss: 0.3128\n",
            "180/350 --- Loss: 0.3126\n",
            "181/350 --- Loss: 0.3123\n",
            "182/350 --- Loss: 0.3121\n",
            "183/350 --- Loss: 0.3119\n",
            "184/350 --- Loss: 0.3117\n",
            "185/350 --- Loss: 0.3114\n",
            "186/350 --- Loss: 0.3112\n",
            "187/350 --- Loss: 0.3110\n",
            "188/350 --- Loss: 0.3108\n",
            "189/350 --- Loss: 0.3105\n",
            "190/350 --- Loss: 0.3103\n",
            "191/350 --- Loss: 0.3101\n",
            "192/350 --- Loss: 0.3099\n",
            "193/350 --- Loss: 0.3097\n",
            "194/350 --- Loss: 0.3095\n",
            "195/350 --- Loss: 0.3093\n",
            "196/350 --- Loss: 0.3090\n",
            "197/350 --- Loss: 0.3088\n",
            "198/350 --- Loss: 0.3086\n",
            "199/350 --- Loss: 0.3084\n",
            "200/350 --- Loss: 0.3082\n",
            "201/350 --- Loss: 0.3080\n",
            "202/350 --- Loss: 0.3078\n",
            "203/350 --- Loss: 0.3076\n",
            "204/350 --- Loss: 0.3074\n",
            "205/350 --- Loss: 0.3072\n",
            "206/350 --- Loss: 0.3070\n",
            "207/350 --- Loss: 0.3068\n",
            "208/350 --- Loss: 0.3066\n",
            "209/350 --- Loss: 0.3064\n",
            "210/350 --- Loss: 0.3062\n",
            "211/350 --- Loss: 0.3060\n",
            "212/350 --- Loss: 0.3058\n",
            "213/350 --- Loss: 0.3056\n",
            "214/350 --- Loss: 0.3055\n",
            "215/350 --- Loss: 0.3053\n",
            "216/350 --- Loss: 0.3051\n",
            "217/350 --- Loss: 0.3049\n",
            "218/350 --- Loss: 0.3047\n",
            "219/350 --- Loss: 0.3045\n",
            "220/350 --- Loss: 0.3043\n",
            "221/350 --- Loss: 0.3042\n",
            "222/350 --- Loss: 0.3040\n",
            "223/350 --- Loss: 0.3038\n",
            "224/350 --- Loss: 0.3036\n",
            "225/350 --- Loss: 0.3034\n",
            "226/350 --- Loss: 0.3032\n",
            "227/350 --- Loss: 0.3031\n",
            "228/350 --- Loss: 0.3029\n",
            "229/350 --- Loss: 0.3027\n",
            "230/350 --- Loss: 0.3025\n",
            "231/350 --- Loss: 0.3024\n",
            "232/350 --- Loss: 0.3022\n",
            "233/350 --- Loss: 0.3020\n",
            "234/350 --- Loss: 0.3018\n",
            "235/350 --- Loss: 0.3017\n",
            "236/350 --- Loss: 0.3015\n",
            "237/350 --- Loss: 0.3013\n",
            "238/350 --- Loss: 0.3012\n",
            "239/350 --- Loss: 0.3010\n",
            "240/350 --- Loss: 0.3008\n",
            "241/350 --- Loss: 0.3007\n",
            "242/350 --- Loss: 0.3005\n",
            "243/350 --- Loss: 0.3003\n",
            "244/350 --- Loss: 0.3002\n",
            "245/350 --- Loss: 0.3000\n",
            "246/350 --- Loss: 0.2998\n",
            "247/350 --- Loss: 0.2997\n",
            "248/350 --- Loss: 0.2995\n",
            "249/350 --- Loss: 0.2994\n",
            "250/350 --- Loss: 0.2992\n",
            "251/350 --- Loss: 0.2990\n",
            "252/350 --- Loss: 0.2989\n",
            "253/350 --- Loss: 0.2987\n",
            "254/350 --- Loss: 0.2986\n",
            "255/350 --- Loss: 0.2984\n",
            "256/350 --- Loss: 0.2982\n",
            "257/350 --- Loss: 0.2981\n",
            "258/350 --- Loss: 0.2979\n",
            "259/350 --- Loss: 0.2978\n",
            "260/350 --- Loss: 0.2976\n",
            "261/350 --- Loss: 0.2975\n",
            "262/350 --- Loss: 0.2973\n",
            "263/350 --- Loss: 0.2972\n",
            "264/350 --- Loss: 0.2970\n",
            "265/350 --- Loss: 0.2969\n",
            "266/350 --- Loss: 0.2967\n",
            "267/350 --- Loss: 0.2966\n",
            "268/350 --- Loss: 0.2964\n",
            "269/350 --- Loss: 0.2963\n",
            "270/350 --- Loss: 0.2961\n",
            "271/350 --- Loss: 0.2960\n",
            "272/350 --- Loss: 0.2958\n",
            "273/350 --- Loss: 0.2957\n",
            "274/350 --- Loss: 0.2955\n",
            "275/350 --- Loss: 0.2954\n",
            "276/350 --- Loss: 0.2952\n",
            "277/350 --- Loss: 0.2951\n",
            "278/350 --- Loss: 0.2950\n",
            "279/350 --- Loss: 0.2948\n",
            "280/350 --- Loss: 0.2947\n",
            "281/350 --- Loss: 0.2945\n",
            "282/350 --- Loss: 0.2944\n",
            "283/350 --- Loss: 0.2942\n",
            "284/350 --- Loss: 0.2941\n",
            "285/350 --- Loss: 0.2940\n",
            "286/350 --- Loss: 0.2938\n",
            "287/350 --- Loss: 0.2937\n",
            "288/350 --- Loss: 0.2935\n",
            "289/350 --- Loss: 0.2934\n",
            "290/350 --- Loss: 0.2933\n",
            "291/350 --- Loss: 0.2931\n",
            "292/350 --- Loss: 0.2930\n",
            "293/350 --- Loss: 0.2928\n",
            "294/350 --- Loss: 0.2927\n",
            "295/350 --- Loss: 0.2926\n",
            "296/350 --- Loss: 0.2924\n",
            "297/350 --- Loss: 0.2923\n",
            "298/350 --- Loss: 0.2922\n",
            "299/350 --- Loss: 0.2920\n",
            "300/350 --- Loss: 0.2919\n",
            "301/350 --- Loss: 0.2918\n",
            "302/350 --- Loss: 0.2916\n",
            "303/350 --- Loss: 0.2915\n",
            "304/350 --- Loss: 0.2914\n",
            "305/350 --- Loss: 0.2912\n",
            "306/350 --- Loss: 0.2911\n",
            "307/350 --- Loss: 0.2910\n",
            "308/350 --- Loss: 0.2908\n",
            "309/350 --- Loss: 0.2907\n",
            "310/350 --- Loss: 0.2906\n",
            "311/350 --- Loss: 0.2905\n",
            "312/350 --- Loss: 0.2903\n",
            "313/350 --- Loss: 0.2902\n",
            "314/350 --- Loss: 0.2901\n",
            "315/350 --- Loss: 0.2899\n",
            "316/350 --- Loss: 0.2898\n",
            "317/350 --- Loss: 0.2897\n",
            "318/350 --- Loss: 0.2896\n",
            "319/350 --- Loss: 0.2894\n",
            "320/350 --- Loss: 0.2893\n",
            "321/350 --- Loss: 0.2892\n",
            "322/350 --- Loss: 0.2891\n",
            "323/350 --- Loss: 0.2889\n",
            "324/350 --- Loss: 0.2888\n",
            "325/350 --- Loss: 0.2887\n",
            "326/350 --- Loss: 0.2886\n",
            "327/350 --- Loss: 0.2884\n",
            "328/350 --- Loss: 0.2883\n",
            "329/350 --- Loss: 0.2882\n",
            "330/350 --- Loss: 0.2881\n",
            "331/350 --- Loss: 0.2879\n",
            "332/350 --- Loss: 0.2878\n",
            "333/350 --- Loss: 0.2877\n",
            "334/350 --- Loss: 0.2876\n",
            "335/350 --- Loss: 0.2875\n",
            "336/350 --- Loss: 0.2873\n",
            "337/350 --- Loss: 0.2872\n",
            "338/350 --- Loss: 0.2871\n",
            "339/350 --- Loss: 0.2870\n",
            "340/350 --- Loss: 0.2869\n",
            "341/350 --- Loss: 0.2867\n",
            "342/350 --- Loss: 0.2866\n",
            "343/350 --- Loss: 0.2865\n",
            "344/350 --- Loss: 0.2864\n",
            "345/350 --- Loss: 0.2863\n",
            "346/350 --- Loss: 0.2861\n",
            "347/350 --- Loss: 0.2860\n",
            "348/350 --- Loss: 0.2859\n",
            "349/350 --- Loss: 0.2858\n",
            "350/350 --- Loss: 0.2857\n",
            "\n",
            " Layer 3 : \n",
            "1/350 --- Loss: 0.8048\n",
            "2/350 --- Loss: 0.5744\n",
            "3/350 --- Loss: 0.6659\n",
            "4/350 --- Loss: 0.6074\n",
            "5/350 --- Loss: 0.5340\n",
            "6/350 --- Loss: 0.5312\n",
            "7/350 --- Loss: 0.5477\n",
            "8/350 --- Loss: 0.5421\n",
            "9/350 --- Loss: 0.5158\n",
            "10/350 --- Loss: 0.4898\n",
            "11/350 --- Loss: 0.4836\n",
            "12/350 --- Loss: 0.4939\n",
            "13/350 --- Loss: 0.4990\n",
            "14/350 --- Loss: 0.4891\n",
            "15/350 --- Loss: 0.4731\n",
            "16/350 --- Loss: 0.4640\n",
            "17/350 --- Loss: 0.4648\n",
            "18/350 --- Loss: 0.4686\n",
            "19/350 --- Loss: 0.4677\n",
            "20/350 --- Loss: 0.4611\n",
            "21/350 --- Loss: 0.4533\n",
            "22/350 --- Loss: 0.4494\n",
            "23/350 --- Loss: 0.4500\n",
            "24/350 --- Loss: 0.4512\n",
            "25/350 --- Loss: 0.4495\n",
            "26/350 --- Loss: 0.4451\n",
            "27/350 --- Loss: 0.4412\n",
            "28/350 --- Loss: 0.4396\n",
            "29/350 --- Loss: 0.4395\n",
            "30/350 --- Loss: 0.4389\n",
            "31/350 --- Loss: 0.4370\n",
            "32/350 --- Loss: 0.4345\n",
            "33/350 --- Loss: 0.4327\n",
            "34/350 --- Loss: 0.4319\n",
            "35/350 --- Loss: 0.4314\n",
            "36/350 --- Loss: 0.4304\n",
            "37/350 --- Loss: 0.4289\n",
            "38/350 --- Loss: 0.4274\n",
            "39/350 --- Loss: 0.4264\n",
            "40/350 --- Loss: 0.4256\n",
            "41/350 --- Loss: 0.4248\n",
            "42/350 --- Loss: 0.4238\n",
            "43/350 --- Loss: 0.4228\n",
            "44/350 --- Loss: 0.4219\n",
            "45/350 --- Loss: 0.4211\n",
            "46/350 --- Loss: 0.4204\n",
            "47/350 --- Loss: 0.4196\n",
            "48/350 --- Loss: 0.4188\n",
            "49/350 --- Loss: 0.4180\n",
            "50/350 --- Loss: 0.4173\n",
            "51/350 --- Loss: 0.4166\n",
            "52/350 --- Loss: 0.4159\n",
            "53/350 --- Loss: 0.4152\n",
            "54/350 --- Loss: 0.4146\n",
            "55/350 --- Loss: 0.4140\n",
            "56/350 --- Loss: 0.4134\n",
            "57/350 --- Loss: 0.4127\n",
            "58/350 --- Loss: 0.4121\n",
            "59/350 --- Loss: 0.4115\n",
            "60/350 --- Loss: 0.4110\n",
            "61/350 --- Loss: 0.4104\n",
            "62/350 --- Loss: 0.4098\n",
            "63/350 --- Loss: 0.4093\n",
            "64/350 --- Loss: 0.4088\n",
            "65/350 --- Loss: 0.4083\n",
            "66/350 --- Loss: 0.4077\n",
            "67/350 --- Loss: 0.4072\n",
            "68/350 --- Loss: 0.4068\n",
            "69/350 --- Loss: 0.4063\n",
            "70/350 --- Loss: 0.4058\n",
            "71/350 --- Loss: 0.4053\n",
            "72/350 --- Loss: 0.4048\n",
            "73/350 --- Loss: 0.4044\n",
            "74/350 --- Loss: 0.4039\n",
            "75/350 --- Loss: 0.4035\n",
            "76/350 --- Loss: 0.4031\n",
            "77/350 --- Loss: 0.4026\n",
            "78/350 --- Loss: 0.4022\n",
            "79/350 --- Loss: 0.4018\n",
            "80/350 --- Loss: 0.4014\n",
            "81/350 --- Loss: 0.4010\n",
            "82/350 --- Loss: 0.4006\n",
            "83/350 --- Loss: 0.4001\n",
            "84/350 --- Loss: 0.3998\n",
            "85/350 --- Loss: 0.3994\n",
            "86/350 --- Loss: 0.3990\n",
            "87/350 --- Loss: 0.3986\n",
            "88/350 --- Loss: 0.3983\n",
            "89/350 --- Loss: 0.3979\n",
            "90/350 --- Loss: 0.3975\n",
            "91/350 --- Loss: 0.3972\n",
            "92/350 --- Loss: 0.3968\n",
            "93/350 --- Loss: 0.3965\n",
            "94/350 --- Loss: 0.3961\n",
            "95/350 --- Loss: 0.3958\n",
            "96/350 --- Loss: 0.3955\n",
            "97/350 --- Loss: 0.3951\n",
            "98/350 --- Loss: 0.3948\n",
            "99/350 --- Loss: 0.3945\n",
            "100/350 --- Loss: 0.3942\n",
            "101/350 --- Loss: 0.3939\n",
            "102/350 --- Loss: 0.3936\n",
            "103/350 --- Loss: 0.3932\n",
            "104/350 --- Loss: 0.3929\n",
            "105/350 --- Loss: 0.3926\n",
            "106/350 --- Loss: 0.3924\n",
            "107/350 --- Loss: 0.3921\n",
            "108/350 --- Loss: 0.3918\n",
            "109/350 --- Loss: 0.3915\n",
            "110/350 --- Loss: 0.3912\n",
            "111/350 --- Loss: 0.3909\n",
            "112/350 --- Loss: 0.3906\n",
            "113/350 --- Loss: 0.3904\n",
            "114/350 --- Loss: 0.3901\n",
            "115/350 --- Loss: 0.3898\n",
            "116/350 --- Loss: 0.3896\n",
            "117/350 --- Loss: 0.3893\n",
            "118/350 --- Loss: 0.3891\n",
            "119/350 --- Loss: 0.3888\n",
            "120/350 --- Loss: 0.3885\n",
            "121/350 --- Loss: 0.3883\n",
            "122/350 --- Loss: 0.3880\n",
            "123/350 --- Loss: 0.3878\n",
            "124/350 --- Loss: 0.3876\n",
            "125/350 --- Loss: 0.3873\n",
            "126/350 --- Loss: 0.3871\n",
            "127/350 --- Loss: 0.3868\n",
            "128/350 --- Loss: 0.3866\n",
            "129/350 --- Loss: 0.3864\n",
            "130/350 --- Loss: 0.3861\n",
            "131/350 --- Loss: 0.3859\n",
            "132/350 --- Loss: 0.3857\n",
            "133/350 --- Loss: 0.3855\n",
            "134/350 --- Loss: 0.3853\n",
            "135/350 --- Loss: 0.3850\n",
            "136/350 --- Loss: 0.3848\n",
            "137/350 --- Loss: 0.3846\n",
            "138/350 --- Loss: 0.3844\n",
            "139/350 --- Loss: 0.3842\n",
            "140/350 --- Loss: 0.3840\n",
            "141/350 --- Loss: 0.3838\n",
            "142/350 --- Loss: 0.3836\n",
            "143/350 --- Loss: 0.3834\n",
            "144/350 --- Loss: 0.3832\n",
            "145/350 --- Loss: 0.3830\n",
            "146/350 --- Loss: 0.3828\n",
            "147/350 --- Loss: 0.3826\n",
            "148/350 --- Loss: 0.3824\n",
            "149/350 --- Loss: 0.3822\n",
            "150/350 --- Loss: 0.3820\n",
            "151/350 --- Loss: 0.3818\n",
            "152/350 --- Loss: 0.3816\n",
            "153/350 --- Loss: 0.3814\n",
            "154/350 --- Loss: 0.3812\n",
            "155/350 --- Loss: 0.3810\n",
            "156/350 --- Loss: 0.3809\n",
            "157/350 --- Loss: 0.3807\n",
            "158/350 --- Loss: 0.3805\n",
            "159/350 --- Loss: 0.3803\n",
            "160/350 --- Loss: 0.3802\n",
            "161/350 --- Loss: 0.3800\n",
            "162/350 --- Loss: 0.3798\n",
            "163/350 --- Loss: 0.3796\n",
            "164/350 --- Loss: 0.3795\n",
            "165/350 --- Loss: 0.3793\n",
            "166/350 --- Loss: 0.3791\n",
            "167/350 --- Loss: 0.3790\n",
            "168/350 --- Loss: 0.3788\n",
            "169/350 --- Loss: 0.3786\n",
            "170/350 --- Loss: 0.3785\n",
            "171/350 --- Loss: 0.3783\n",
            "172/350 --- Loss: 0.3781\n",
            "173/350 --- Loss: 0.3780\n",
            "174/350 --- Loss: 0.3778\n",
            "175/350 --- Loss: 0.3777\n",
            "176/350 --- Loss: 0.3775\n",
            "177/350 --- Loss: 0.3774\n",
            "178/350 --- Loss: 0.3772\n",
            "179/350 --- Loss: 0.3771\n",
            "180/350 --- Loss: 0.3769\n",
            "181/350 --- Loss: 0.3767\n",
            "182/350 --- Loss: 0.3766\n",
            "183/350 --- Loss: 0.3765\n",
            "184/350 --- Loss: 0.3763\n",
            "185/350 --- Loss: 0.3762\n",
            "186/350 --- Loss: 0.3760\n",
            "187/350 --- Loss: 0.3759\n",
            "188/350 --- Loss: 0.3757\n",
            "189/350 --- Loss: 0.3756\n",
            "190/350 --- Loss: 0.3754\n",
            "191/350 --- Loss: 0.3753\n",
            "192/350 --- Loss: 0.3752\n",
            "193/350 --- Loss: 0.3750\n",
            "194/350 --- Loss: 0.3749\n",
            "195/350 --- Loss: 0.3747\n",
            "196/350 --- Loss: 0.3746\n",
            "197/350 --- Loss: 0.3745\n",
            "198/350 --- Loss: 0.3743\n",
            "199/350 --- Loss: 0.3742\n",
            "200/350 --- Loss: 0.3741\n",
            "201/350 --- Loss: 0.3739\n",
            "202/350 --- Loss: 0.3738\n",
            "203/350 --- Loss: 0.3737\n",
            "204/350 --- Loss: 0.3736\n",
            "205/350 --- Loss: 0.3734\n",
            "206/350 --- Loss: 0.3733\n",
            "207/350 --- Loss: 0.3732\n",
            "208/350 --- Loss: 0.3730\n",
            "209/350 --- Loss: 0.3729\n",
            "210/350 --- Loss: 0.3728\n",
            "211/350 --- Loss: 0.3727\n",
            "212/350 --- Loss: 0.3725\n",
            "213/350 --- Loss: 0.3724\n",
            "214/350 --- Loss: 0.3723\n",
            "215/350 --- Loss: 0.3722\n",
            "216/350 --- Loss: 0.3721\n",
            "217/350 --- Loss: 0.3719\n",
            "218/350 --- Loss: 0.3718\n",
            "219/350 --- Loss: 0.3717\n",
            "220/350 --- Loss: 0.3716\n",
            "221/350 --- Loss: 0.3715\n",
            "222/350 --- Loss: 0.3714\n",
            "223/350 --- Loss: 0.3712\n",
            "224/350 --- Loss: 0.3711\n",
            "225/350 --- Loss: 0.3710\n",
            "226/350 --- Loss: 0.3709\n",
            "227/350 --- Loss: 0.3708\n",
            "228/350 --- Loss: 0.3707\n",
            "229/350 --- Loss: 0.3706\n",
            "230/350 --- Loss: 0.3704\n",
            "231/350 --- Loss: 0.3703\n",
            "232/350 --- Loss: 0.3702\n",
            "233/350 --- Loss: 0.3701\n",
            "234/350 --- Loss: 0.3700\n",
            "235/350 --- Loss: 0.3699\n",
            "236/350 --- Loss: 0.3698\n",
            "237/350 --- Loss: 0.3697\n",
            "238/350 --- Loss: 0.3696\n",
            "239/350 --- Loss: 0.3695\n",
            "240/350 --- Loss: 0.3694\n",
            "241/350 --- Loss: 0.3693\n",
            "242/350 --- Loss: 0.3691\n",
            "243/350 --- Loss: 0.3690\n",
            "244/350 --- Loss: 0.3689\n",
            "245/350 --- Loss: 0.3688\n",
            "246/350 --- Loss: 0.3687\n",
            "247/350 --- Loss: 0.3686\n",
            "248/350 --- Loss: 0.3685\n",
            "249/350 --- Loss: 0.3684\n",
            "250/350 --- Loss: 0.3683\n",
            "251/350 --- Loss: 0.3682\n",
            "252/350 --- Loss: 0.3681\n",
            "253/350 --- Loss: 0.3680\n",
            "254/350 --- Loss: 0.3679\n",
            "255/350 --- Loss: 0.3678\n",
            "256/350 --- Loss: 0.3677\n",
            "257/350 --- Loss: 0.3676\n",
            "258/350 --- Loss: 0.3675\n",
            "259/350 --- Loss: 0.3674\n",
            "260/350 --- Loss: 0.3673\n",
            "261/350 --- Loss: 0.3672\n",
            "262/350 --- Loss: 0.3671\n",
            "263/350 --- Loss: 0.3671\n",
            "264/350 --- Loss: 0.3670\n",
            "265/350 --- Loss: 0.3669\n",
            "266/350 --- Loss: 0.3668\n",
            "267/350 --- Loss: 0.3667\n",
            "268/350 --- Loss: 0.3666\n",
            "269/350 --- Loss: 0.3665\n",
            "270/350 --- Loss: 0.3664\n",
            "271/350 --- Loss: 0.3663\n",
            "272/350 --- Loss: 0.3662\n",
            "273/350 --- Loss: 0.3661\n",
            "274/350 --- Loss: 0.3660\n",
            "275/350 --- Loss: 0.3659\n",
            "276/350 --- Loss: 0.3658\n",
            "277/350 --- Loss: 0.3657\n",
            "278/350 --- Loss: 0.3656\n",
            "279/350 --- Loss: 0.3656\n",
            "280/350 --- Loss: 0.3655\n",
            "281/350 --- Loss: 0.3654\n",
            "282/350 --- Loss: 0.3653\n",
            "283/350 --- Loss: 0.3652\n",
            "284/350 --- Loss: 0.3651\n",
            "285/350 --- Loss: 0.3650\n",
            "286/350 --- Loss: 0.3649\n",
            "287/350 --- Loss: 0.3648\n",
            "288/350 --- Loss: 0.3648\n",
            "289/350 --- Loss: 0.3647\n",
            "290/350 --- Loss: 0.3646\n",
            "291/350 --- Loss: 0.3645\n",
            "292/350 --- Loss: 0.3644\n",
            "293/350 --- Loss: 0.3643\n",
            "294/350 --- Loss: 0.3642\n",
            "295/350 --- Loss: 0.3641\n",
            "296/350 --- Loss: 0.3641\n",
            "297/350 --- Loss: 0.3640\n",
            "298/350 --- Loss: 0.3639\n",
            "299/350 --- Loss: 0.3638\n",
            "300/350 --- Loss: 0.3637\n",
            "301/350 --- Loss: 0.3636\n",
            "302/350 --- Loss: 0.3635\n",
            "303/350 --- Loss: 0.3635\n",
            "304/350 --- Loss: 0.3634\n",
            "305/350 --- Loss: 0.3633\n",
            "306/350 --- Loss: 0.3632\n",
            "307/350 --- Loss: 0.3631\n",
            "308/350 --- Loss: 0.3630\n",
            "309/350 --- Loss: 0.3630\n",
            "310/350 --- Loss: 0.3629\n",
            "311/350 --- Loss: 0.3628\n",
            "312/350 --- Loss: 0.3627\n",
            "313/350 --- Loss: 0.3626\n",
            "314/350 --- Loss: 0.3626\n",
            "315/350 --- Loss: 0.3625\n",
            "316/350 --- Loss: 0.3624\n",
            "317/350 --- Loss: 0.3623\n",
            "318/350 --- Loss: 0.3622\n",
            "319/350 --- Loss: 0.3622\n",
            "320/350 --- Loss: 0.3621\n",
            "321/350 --- Loss: 0.3620\n",
            "322/350 --- Loss: 0.3619\n",
            "323/350 --- Loss: 0.3618\n",
            "324/350 --- Loss: 0.3618\n",
            "325/350 --- Loss: 0.3617\n",
            "326/350 --- Loss: 0.3616\n",
            "327/350 --- Loss: 0.3615\n",
            "328/350 --- Loss: 0.3615\n",
            "329/350 --- Loss: 0.3614\n",
            "330/350 --- Loss: 0.3613\n",
            "331/350 --- Loss: 0.3612\n",
            "332/350 --- Loss: 0.3612\n",
            "333/350 --- Loss: 0.3611\n",
            "334/350 --- Loss: 0.3610\n",
            "335/350 --- Loss: 0.3609\n",
            "336/350 --- Loss: 0.3609\n",
            "337/350 --- Loss: 0.3608\n",
            "338/350 --- Loss: 0.3607\n",
            "339/350 --- Loss: 0.3606\n",
            "340/350 --- Loss: 0.3606\n",
            "341/350 --- Loss: 0.3605\n",
            "342/350 --- Loss: 0.3604\n",
            "343/350 --- Loss: 0.3603\n",
            "344/350 --- Loss: 0.3603\n",
            "345/350 --- Loss: 0.3602\n",
            "346/350 --- Loss: 0.3601\n",
            "347/350 --- Loss: 0.3600\n",
            "348/350 --- Loss: 0.3600\n",
            "349/350 --- Loss: 0.3599\n",
            "350/350 --- Loss: 0.3598\n",
            "--------------------------------------------------\n",
            "\n",
            "train accuracy: 92.14 %\n",
            "test accuracy: 90.70 %\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 350\n",
        "lr = 0.15\n",
        "threshold = 2\n",
        "hidden_dims = [784, 1000, 500, 200]\n",
        "model = FF(hidden_dims ,device=device,num_epochs=num_epochs, lr = lr, threshold=threshold)\n",
        "x_train, labels_train = next(iter(train_loader))\n",
        "x_train = x_train.to(device)\n",
        "labels_train = labels_train.to(device)\n",
        "pos_data = model.Generate_data(x_train, labels_train)\n",
        "inx = torch.randint(low=0,high=x_train.shape[0],size = (x_train.shape[0],))\n",
        "neg_data  = model.Generate_data(x_train,labels_train[inx])\n",
        "model.learn(pos_data,neg_data)\n",
        "\n",
        "print(\"--------------------------------------------------\")\n",
        "print()\n",
        "train_accuracy = 100*(model.predict(x_train) == (labels_train)).sum()/x_train.shape[0]\n",
        "print(f\"train accuracy: {train_accuracy.item():.2f} %\")\n",
        "\n",
        "x_ts, y_ts = next(iter(test_loader))\n",
        "x_ts = x_ts.to(device)\n",
        "y_ts = y_ts.to(device)\n",
        "\n",
        "test_accuracy = 100*(model.predict(x_ts) == (y_ts)).sum()/x_ts.shape[0]\n",
        "print(f\"test accuracy: {test_accuracy.item():.2f} %\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcBT5Ct__3rj"
      },
      "source": [
        "### Inspired by ----> YouTube Channel: **Data Scholar**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JXq6NbnrRL_6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
